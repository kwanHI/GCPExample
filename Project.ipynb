{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Notebook\n",
    "\n",
    "### NOTE: If you build and run a notebook in the cloud, just copy it down in place of this one!  \n",
    "#### Be sure to have all your output captured within the notebook!\n",
    "#### <span style=\"background:yellow\">Be sure to save your work early and often!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Specific_Project_1.png MISSING](../images/Specific_Project_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run instance\n",
    "gcloud beta compute --project=cloud15 instances create dsa-mini-project-lcn1055-mod8 --zone=us-central1-a --machine-type=f1-micro --subnet=default --network-tier=PREMIUM --no-restart-on-failure --maintenance-policy=TERMINATE --preemptible --service-account=1014104180905-compute@developer.gserviceaccount.com --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append --image=debian-10-buster-v20210609 --image-project=debian-cloud --boot-disk-size=10GB --boot-disk-type=pd-balanced --boot-disk-device-name=dsa-mini-project-lcn1055-1 --no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --labels=module=6,user=lcn1055 --reservation-affinity=any\n",
    "# I stop and allow api to access before start again and ssh in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install python3-venv python3-pip\n",
    "sudo apt install wget\n",
    "wget https://bootstrap.pypa.io/get-pip.py\n",
    "sudo python3 get-pip.py\n",
    "#create virtual env to install need libraries\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install --upgrade setuptools\n",
    "pip3 install --upgrade google-api-python-client\n",
    "pip3 install --upgrade google-cloud-storage\n",
    "pip3 install --upgrade feedparser\n",
    "pip3 install --upgrade beautifulsoup4\n",
    "pip3 install --upgrade datetime\n",
    "\n",
    "pip3 install --no-cache-dir --force-reinstall grpcio\n",
    "#pip3 install google-cloud-storage #pip3 install --upgrade google-cloud-datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Next, we need to run gcloud to authorize API from the instance's terminal\n",
    "gcloud auth application-default login\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add code as needed in the cells below to produce your analytical products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Include whichever API are appropriate for your cloud provider\n",
    "\n",
    "####################################\n",
    "# File: redditfeed.py\n",
    "####################################\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# for bucket\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# for noSQL Firestore Datastore\n",
    "import googleapiclient.discovery\n",
    "from google.cloud import datastore\n",
    "import requests\n",
    "\n",
    "\n",
    "##Write ReadReddit To Retriev Dictionary DataSET\n",
    "\n",
    "# Functions from: https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "# Define URL of the RSS Feed I want\n",
    "def readRedditDict():\n",
    "    a_reddit_rss_url = 'http://www.reddit.com/new/.rss?sort=new'\n",
    "\n",
    "    feed = feedparser.parse( a_reddit_rss_url )\n",
    "    results = {}\n",
    "    seq =0\n",
    "    if (feed['bozo'] == 1):\n",
    "        print(\"Error Reading/Parsing Feed XML Data\")    \n",
    "    else:\n",
    "        for item in feed[ \"items\" ]:\n",
    "            dttm = item[ \"date\" ] #\"2021-06-17T18:50:49+00:00_2\"\n",
    "            seq +=1\n",
    "            seqdttm = dttm[:4]+dttm[5:7]+dttm[8:10]+dttm[11:13]+dttm[14:16]+dttm[17:19]+\"_\"+str(seq)\n",
    "            results[seqdttm]= \\\n",
    "                    {\n",
    "                    \"posteddatetime\":item[ \"date\" ] ,\n",
    "                    \"title\":item[ \"title\" ],\n",
    "                    \"summary\":text_from_html(item[ \"summary\" ]),\n",
    "                    \"link\":item[ \"link\" ]\n",
    "                    }\n",
    "\n",
    "    return results     \n",
    "\n",
    "############################################\n",
    "##Write ReadReddit To Retriev Dictionary DataSET\n",
    "\n",
    "PROJECT='cloud15'\n",
    "\n",
    "#vbuckname = 'dsa_mini_project_lcn1055'\n",
    "#filename = 'RedditJson_'+datetime.now().strftime(\"%Y%m%d%H%M\")  #length=19\n",
    "\n",
    "\n",
    "\n",
    "def list_blobs(bucket_name, blobnamepattern):\n",
    "    \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    blobs = bucket.list_blobs()\n",
    "    retblobs = []\n",
    "    for blob in blobs:\n",
    "        if blobnamepattern == '':\n",
    "            print(blob.name)\n",
    "            retblobs.append(blob.name)\n",
    "        else:\n",
    "            if blobnamepattern in blob.name:\n",
    "                retblobs.append(blob.name)\n",
    "    return retblobs    \n",
    "        \n",
    "\n",
    "def list_blobs_with_prefix(bucket_name, prefix, delimiter=None):\n",
    "    \"\"\"Lists all the blobs in the bucket that begin with the prefix.\n",
    "    This can be used to list all blobs in a \"folder\", e.g. \"public/\".\n",
    "    The delimiter argument can be used to restrict the results to only the\n",
    "    \"files\" in the given \"folder\". Without the delimiter, the entire tree under\n",
    "    the prefix is returned. For example, given these blobs:\n",
    "        /a/1.txt\n",
    "        /a/b/2.txt\n",
    "    If you just specify prefix = '/a', you'll get back:\n",
    "        /a/1.txt\n",
    "        /a/b/2.txt\n",
    "    However, if you specify prefix='/a' and delimiter='/', you'll get back:\n",
    "        /a/1.txt\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    blobs = bucket.list_blobs(prefix=prefix, delimiter=delimiter)\n",
    "\n",
    "    print('Blobs:')\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "\n",
    "    if delimiter:\n",
    "        print('Prefixes:')\n",
    "        for prefix in blobs.prefixes:\n",
    "            print(prefix)\n",
    "            \n",
    "\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print('File {} uploaded to {}.'.format(\n",
    "        source_file_name,\n",
    "        destination_blob_name))\n",
    "\n",
    "\n",
    "def upload_as_blob(bucket_name, source_data, destination_blob_name, content_type='text/plain'):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    \"\"\"blob.upload_from_filename(source_file_name)\"\"\"\n",
    "    blob.upload_from_string(source_data, content_type=content_type)\n",
    "    \n",
    "    print('Data uploaded to {}.'.format(destination_blob_name))\n",
    "\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print('Blob {} downloaded to {}.'.format(\n",
    "        source_blob_name,\n",
    "        destination_file_name))\n",
    "\n",
    "def delete_blob(bucket_name, blob_name):\n",
    "    \"\"\"Deletes a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.delete()\n",
    "\n",
    "    print('Blob {} deleted.'.format(blob_name))\n",
    "\n",
    "\n",
    "\n",
    "def read_blob_dict(bucket_name, source_blob_name):\n",
    "    \"\"\"Read a blob from the bucket into Text\"\"\"\n",
    "    \n",
    "    storage_client = storage.Client(project=PROJECT)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    # download as string\n",
    "    json_data = bucket.get_blob(source_blob_name).download_as_string()\n",
    "    json_dict = json.loads(json_data.decode('utf-8'))   \n",
    "\n",
    "    return json_dict\n",
    "    \"\"\"\n",
    "    #blob = bucket.get_blob(source_blob_name)\n",
    "    with open(json_data) as f:\n",
    "        data = json.load(json_data)\n",
    "       itSourceId'] = data['RedditSourceId']\n",
    "    \"\"\"\n",
    "## Write your additional Code Segments here, \n",
    "##  because your VM will get deleted\n",
    "\n",
    "\n",
    "vbuckname = 'dsa_mini_project_lcn1055'\n",
    "filename = 'RedditJson_' + datetime.now().strftime(\"%Y%m%d%H%M\")  #length=19\n",
    "list_blobs(vbuckname,'')\n",
    "print('Before load ---------')\n",
    "# upload_as_blob(vbuckname, json.dumps(readRedditDict(), indent = 4), filename)\n",
    "list_blobs(vbuckname,'')\n",
    "print('After load ---------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# File: bucket2NoSQL.py\n",
    "#######################\n",
    "import os\n",
    "from google.cloud import storage\n",
    "import json\n",
    "# for noSQL Firestore Datastore\n",
    "import googleapiclient.discovery\n",
    "from google.cloud import datastore\n",
    "import requests\n",
    "\n",
    "from redditfeed import list_blobs, read_blob_dict\n",
    "######################################\n",
    "\n",
    "#projectid=cloud15\n",
    "\n",
    "def create_client(project_id):\n",
    "    return datastore.Client(project_id)\n",
    "\n",
    "def add_redditItem(projectid, redditsourceid, redditsourcedtt, title, summary, link):\n",
    "    client=create_client(projectid)\n",
    "    key = client.key('Reddit')\n",
    "    print('Entity is Reddit')\n",
    "    reddit = datastore.Entity(\n",
    "        key, exclude_from_indexes=['Title','Summary','Link'])\n",
    "\n",
    "    reddit.update({\n",
    "        'RedditSourceId': redditsourceid, #datetime.datetime.utcnow(),\n",
    "        'RedditSourceDtt': redditsourcedtt,\n",
    "        'Title': title,\n",
    "        'Summary':summary,\n",
    "        'Link': link\n",
    "    })\n",
    "\n",
    "    client.put(reddit)\n",
    "\n",
    "    return reddit.key\n",
    "\n",
    "\n",
    "def mark_done(client, reddit_id):\n",
    "    with client.transaction():\n",
    "        key = client.key('Reddit', reddit_id)\n",
    "        task = client.get(key)\n",
    "\n",
    "        if not task:\n",
    "            raise ValueError(\n",
    "                'Reddit {} does not exist.'.format(reddit_id))\n",
    "\n",
    "        reddit['Title'] = 'Done'\n",
    "\n",
    "        client.put(reddit)\n",
    "\n",
    "        \n",
    "def delete_task(client, reddit_id):\n",
    "    key = client.key('Reddit', reddit_id)\n",
    "    client.delete(key)  \n",
    "    \n",
    "\n",
    "def list_tasks(client):\n",
    "    query = client.query(kind='Reddit')\n",
    "    query.order = ['RedditSourceId']\n",
    "\n",
    "    return list(query.fetch())    \n",
    "\n",
    "\n",
    "### Loop through bucket to look for a file RedditJsonXXXX to  store data into NoSQL\n",
    "vbuckname = 'dsa_mini_project_lcn1055'\n",
    "fnamepattern = 'RedditJson_' \n",
    "PROJECT='cloud15'\n",
    "\n",
    "# Listing RedditJson blob name in the bucket in order to put into NoSQL\n",
    "redditjson = list_blobs(vbuckname, fnamepattern)\n",
    "#client = storage.Client(project=PROJECT)\n",
    "#each  bucketname\n",
    "for bname in redditjson:\n",
    "    # Grab data each blb and read row by row\n",
    "    datadict  = read_blob_dict(vbuckname, bname)\n",
    "    stop =0\n",
    "    print('File No.'+ str(stop))\n",
    "    for k,v in datadict.items():\n",
    "        stop+=1\n",
    "    # start write into  firestore-datasore ###\n",
    "        # print(k)\n",
    "        c1=str(k)\n",
    "        \n",
    "        c2=''\n",
    "        c3=''\n",
    "        c4=''\n",
    "        c5=''\n",
    "        # v=\"{'posteddatetime': '2021-06-19T03:07:43+00:00', 'title': 'ASUS TUF Dash 15, 144Hz FHD, RTX 3050 Ti, i7-11370H, 8GB 512GB, $949 - ASUS ROG 13.4\" AMD Ryzen 9, RTX 3050 Ti, 16GB 1TB, $1499 - ASUS TUF Gaming F15, 144Hz FHD, i7-11800H, RTX 3050 Ti, 16GB 512GB, $1199 - all available for order', 'summary': '     submitted by /u/gamersecret2 to r/GamingLaptops   [link]  [comments] ', 'link': 'https://www.reddit.com/r/GamingLaptops/comments/o36rs7/asus_tuf_dash_15_144hz_fhd_rtx_3050_ti_i711370h/'}\"\n",
    "        cntiteminOneFile=0\n",
    "        for kk,vv in v.items():\n",
    "           cntiteminOneFile+=1  \n",
    "           if kk=='RedditSourceId' :\n",
    "               print('****'+v)\n",
    "               c1=vv\n",
    "           elif kk=='posteddatetime' or kk=='datetime':\n",
    "               c2=vv\n",
    "           elif kk=='title':\n",
    "               c3=vv\n",
    "           elif kk=='summary':\n",
    "               c4=vv\n",
    "           elif kk=='link':\n",
    "               c5=vv\n",
    "           else:\n",
    "               print(\"CANNOT FIND:\" + kk)\n",
    "           \n",
    "        print(c1+'\\n'+c2+'\\n'+c3+'\\n'+c4+'\\n'+c5) \n",
    "        add_redditItem(PROJECT,c1,c2,c3,c4,c5)\n",
    "   \n",
    "        #print(cntiteminOneFile)\n",
    "    print('****************************\\n')\n",
    "    #print('\\n'.join(json.dumps(item) for item in json_list))\n",
    "    \n",
    "    # add_redditItem(client, redditsourceid, redditsourcedtt, title, summary, link):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# File: drawSummary.py #\n",
    "\n",
    "# Obj: To write the outut from datastore into csv\n",
    "#######################\n",
    "                                               \n",
    "from google.cloud import datastore\n",
    "import csv\n",
    "# For help authenticating your client, visit\n",
    "# https://cloud.google.com/docs/authentication/getting-started\n",
    "client = datastore.Client()\n",
    "query = client.query(kind=\"Reddit\")\n",
    "query.add_filter(\"RedditSourceId\", \">\", \"20210615\")\n",
    "# query.order = [\"Title\"]\n",
    "result = list(query.fetch())\n",
    "with open('./output.csv','w') as f:\n",
    "    writer =  csv.writer(f)\n",
    "    writer.writerow(['RedditDtt','Title','Link','Summary'])\n",
    "    for i in result:\n",
    "        c1=i['RedditSourceDtt'] \n",
    "        c2=i['Title']\n",
    "        c3=i['Summary']     \n",
    "        c4=i['Link']\n",
    "        data=[c1,c2,c3,c4]\n",
    "        writer.writerow(data) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your submission\n",
    "\n",
    "### Deliverables: \n",
    "   1. This or a replacement Notebook\n",
    "   1. An aggregateion of data in tabular format that conveyes something interesting about the Reddit RSS feed during your scraping.\n",
    "     * The table can be embedded or uploaded into this folder (CSV or Excel)\n",
    "   1. One or more data visualizations\n",
    "\n",
    "Imbed your image into this page by saving your data visualization as: `FINAL_PROJECT_IMAGE.png`  \n",
    "Upload it to the `module8/exercises/` folder.\n",
    "\n",
    "If you need to, change the file type to `.jpg` or `.jpeg` or ... whatever, then update the link in this cell (double click to edit).  \n",
    "Then re-run this markdown cell to see it.\n",
    "\n",
    "![FINAL_PROJECT_IMAGE.png MISSING](./exercises/FINAL_PROJECT_IMAGE.png)\n",
    "\n",
    "---\n",
    "## Summarize in the fields below\n",
    " 1. Describe the overall process and components you used for the project.\n",
    " 2. What is the key insight from the tabularization?\n",
    " 3. What is the key insight from the visualization?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 1. \n",
    "# --------------------------\n",
    "\n",
    "#Set up Instance\n",
    "\n",
    "run instance\n",
    "gcloud beta compute --project=cloud15 instances create dsa-mini-project-lcn1055-mod8 --zone=us-central1-a --machine-type=f1-micro --subnet=default --network-tier=PREMIUM --no-restart-on-failure --maintenance-policy=TERMINATE --preemptible --service-account=1014104180905-compute@developer.gserviceaccount.com --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append --image=debian-10-buster-v20210609 --image-project=debian-cloud --boot-disk-size=10GB --boot-disk-type=pd-balanced --boot-disk-device-name=dsa-mini-project-lcn1055-1 --no-shielded-secure-boot --shielded-vtpm --shielded-integrity-monitoring --labels=module=6,user=lcn1055 --reservation-affinity=any\n",
    "# I stop and allow api to access before start again and ssh in."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 2. \n",
    "# --------------------------\n",
    "# Install all needed software in the instance\n",
    "\n",
    "\n",
    "\n",
    "sudo apt install python3-venv python3-pip\n",
    "sudo apt install wget\n",
    "wget https://bootstrap.pypa.io/get-pip.py\n",
    "sudo python3 get-pip.py\n",
    "#create virtual env to install need libraries\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install --upgrade setuptools\n",
    "pip3 install --upgrade google-api-python-client\n",
    "pip3 install --upgrade google-cloud-storage\n",
    "pip3 install --upgrade feedparser\n",
    "pip3 install --upgrade beautifulsoup4\n",
    "pip3 install --upgrade datetime\n",
    "\n",
    "pip3 install --no-cache-dir --force-reinstall grpcio\n",
    "#pip3 install google-cloud-storage #pip3 install --upgrade google-cloud-datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.\n",
    "# --------------------------\n",
    "\n",
    "#Next, we need to run gcloud to authorize API from the instance's terminal\n",
    "gcloud auth application-default login\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "# --------------------------\n",
    "\n",
    "# Set up Google Firestore DataStore and create an entity with 5 columns\n",
    "#Entity: Reddit\n",
    "#Column: RedditSourceId, RedditSourceDatetime, Title, Summary, Link\n",
    "    \n",
    "![Reddit NoSQL Result](../images/Mod8_DataStore_RedditEntity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5\n",
    "# --------------------------\n",
    "\n",
    "# Set up Google Cloudstore Bucket\n",
    "Name: dsa_mini_project_lcn1055\n",
    "# Set up API to allow access\n",
    "\n",
    "![Show Reddit Json Bucket](../images/Mod8_GCPBucket_storingRedditJsonFormat.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6\n",
    "# --------------------------\n",
    "\n",
    "#Next, we need to run gcloud to authorize API from the instance's terminal\n",
    "# In the Instance SSH, I have run bucket2NoSQL.py to read reddit and put into json into the bucket and read to store into NoSQL\n",
    "\n",
    "Filename is redditfeed.py and bucket2NoSQL.py\n",
    "can be found under Exercises Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\n",
    "# --------------------------\n",
    "\n",
    "# Next, We run script to generate Visualization - output.csv from DataScore using \n",
    "# the script called drawSummary.py\n",
    "# Here is the result\n",
    "\n",
    "![Output From DataStore Pull](../images/output_csv_screenshot.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is the key insight from the tabularization?\n",
    "-> Depending on RedditSourceDateTime\n",
    "What is the key insight from the visualization?\n",
    "-> There are 175 records and we can pull based on the column we have set for Indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Save your Notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
